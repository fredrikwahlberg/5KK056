{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Digital Implementations in Heritage - OCR with Tesseract.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOYViKnPk4_"
      },
      "source": [
        "# Fundamentals of machine transcription\n",
        " \n",
        "In this lab, you will play around with image degradation, OCR, and transcription quality. This notebook is written so that all cells can be run at any time (with the obvious exception that you must first load some data before manipulating it). In the \"runtime\" menu above, you can restart your instance of the virtual machine that your code is being run on.\n",
        " \n",
        "There will be no hand-ins or reports for this lab. I consider those necessary evils for labs that should be about experimentation. Put your time into reading the application papers in the slides, play around with this notebook (maybe adding your own text or images), and discuss the technology during the lab and digital office hours.\n",
        " \n",
        "If you want to learn more about programming, the [python website](https://www.python.org/about/gettingstarted/) (which has some introductions for beginners) and [the programming historian](https://programminghistorian.org/en/lessons/). These are also several courses on programming at UU and at other universities. The most important thing might be to find something you would like to do and then learn what's needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Req31bvnlP"
      },
      "source": [
        "## Setting up the virtual machine\r\n",
        " \r\n",
        "The code cell below will install and load all the necessary software for this lab on your current virtual machine on colab. A virtual machine will automatically be created for you when you run something. Simply press play :)\r\n",
        " \r\n",
        "If you want to run this on *your own machine*, you will need to have the packages tesseract (for OCR) and pytesseract (tesseract python bindings) installed. If you are working in Colab, the next code cell will set up your environment. You can install pytesseract by running ```pip3 install --user pytesseract``` in your terminal. More information on the OCR software packages can be found on [pypi](https://pypi.org/project/pytesseract/) and [wikipedia](https://en.wikipedia.org/wiki/Tesseract_(software)). For running you own python environment, I recommend using the [anaconda python distribution](https://www.anaconda.com/products/individual)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w35L7ZaQvoL8"
      },
      "source": [
        "language_models = ['eng', 'swe']\n",
        "\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "  # Base package\n",
        "  !apt install tesseract-ocr\n",
        "  # Swedish language model\n",
        "  for lang in language_models:\n",
        "    if lang != 'eng':\n",
        "      !apt install tesseract-ocr-{lang}\n",
        "  # Python bindings\n",
        "  !pip install pytesseract\n",
        "  # git repo with images and some more code\n",
        "  !git clone https://github.com/fredrikwahlberg/5KK056.git\n",
        "  # Enter the repo folder\n",
        "  %cd /content/5KK056\n",
        "\n",
        "import cv2                        # Computer Vision\n",
        "import numpy as np                # Vector math\n",
        "import pytesseract                # OCR\n",
        "import os.path                    # File system stuff\n",
        "from levenshtein import wer, cer  # Error metrics for strings\n",
        "import ipywidgets as widgets      # For interactivity\n",
        "\n",
        "# We will need a list of the font availible for OpenCV\n",
        "fonts = [(\"FONT_HERSHEY_SIMPLEX\", cv2.FONT_HERSHEY_SIMPLEX),\n",
        "         (\"FONT_HERSHEY_PLAIN\", cv2.FONT_HERSHEY_PLAIN),\n",
        "         (\"FONT_HERSHEY_DUPLEX\", cv2.FONT_HERSHEY_DUPLEX),\n",
        "         (\"FONT_HERSHEY_COMPLEX\", cv2.FONT_HERSHEY_COMPLEX),\n",
        "         (\"FONT_HERSHEY_TRIPLEX\", cv2.FONT_HERSHEY_TRIPLEX),\n",
        "         (\"FONT_HERSHEY_COMPLEX_SMALL\", cv2.FONT_HERSHEY_COMPLEX_SMALL),\n",
        "         (\"FONT_HERSHEY_SCRIPT_SIMPLEX\", cv2.FONT_HERSHEY_SCRIPT_SIMPLEX),\n",
        "         (\"FONT_HERSHEY_SCRIPT_COMPLEX\", cv2.FONT_HERSHEY_SCRIPT_COMPLEX),\n",
        "         (\"FONT_ITALIC\", cv2.FONT_ITALIC)]\n",
        "\n",
        "import matplotlib.pyplot as plt   # Plotting, the line after this one is only for jupyter notebook\n",
        "%matplotlib inline\n",
        "\n",
        "chosen_language_model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXpG13hwvlRu"
      },
      "source": [
        "## Load data\r\n",
        "\r\n",
        "The following cells will load some data for you to work with.\r\n",
        "\r\n",
        "*Note that the following cells will overwrite each other. Running two of them will only keep the latest loaded data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mflxsSt3Pk5H"
      },
      "source": [
        "### Eisenhower's military industrial complex speech\n",
        " \n",
        "A part of Eisenhower's famous speech about the influence of the military industrial complex. These issues are of course solved now after the 2020 election..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KtWGntLPk5P"
      },
      "source": [
        "original_image = cv2.imread(\"Eisenhower.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image = original_image.copy()\n",
        "\n",
        "with open(\"Eisenhower.txt\", 'r') as file:\n",
        "    golden_transcription = file.read()\n",
        "transcription = None\n",
        "\n",
        "print(\"Loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeBqWeiqwjai"
      },
      "source": [
        "### An old encyclopedia entry on Gutenberg\r\n",
        "\r\n",
        "This image is from an old Swedish encyclopedia on Gutenberg. It could be worth thinking about when running the OCR that this is written in Swedish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKItMEIjwjhh"
      },
      "source": [
        "original_image = cv2.imread(\"Gutenberg.png\", cv2.IMREAD_GRAYSCALE)\r\n",
        "image = original_image.copy()\r\n",
        "\r\n",
        "with open(\"Gutenberg.txt\", 'r', encoding='utf-8') as file:\r\n",
        "    golden_transcription = file.read()\r\n",
        "transcription = None\r\n",
        "\r\n",
        "print(\"Loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcE65ioxIOm"
      },
      "source": [
        "### Generate an image from text\r\n",
        " \r\n",
        "This cell generates an image from the text in the ```txt``` variable. Try changing it to something else. If you want to write something in a language other than swedish or english, you will need to load a new language model for OCR quality. You can do this by adding the three letter language code (ISO standard) to the ```language_models``` list in the first code cell then running it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6XBKabPxIUm"
      },
      "source": [
        "golden_transcription = \"\"\"Shall I compare thee to a summer's day?\r\n",
        "Thou art more lovely and more temperate:\r\n",
        "Rough winds do shake the darling buds of May,\r\n",
        "And summer's lease hath all too short a date:\r\n",
        "Sometime too hot the eye of heaven shines,\r\n",
        "And often is his gold complexion dimmed;\r\n",
        "And every fair from fair sometime declines,\r\n",
        "By chance, or nature's changing course, untrimmed:\r\n",
        "But thy eternal summer shall not fade,\r\n",
        "Nor lose possession of that fair thou ow'st;\r\n",
        "Nor shall Death brag thou wander'st in his shade\r\n",
        "When in eternal lines to time thou grow'st:\r\n",
        "So long as men can breathe or eyes can see,\r\n",
        "So long lives this, and this gives life to thee.\"\"\"\r\n",
        "\r\n",
        "def render_text(font, scale):\r\n",
        "  global image, original_image, golden_transcription\r\n",
        "  image = np.zeros((2000, 2000), dtype=np.uint8)\r\n",
        "  # Render text lines\r\n",
        "  y = 100\r\n",
        "  for i, textline in enumerate(golden_transcription.split('\\n')):\r\n",
        "    cv2.putText(image, textline.strip(), (100, y), fontFace=font, fontScale=scale, color=(255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\r\n",
        "    res = cv2.getTextSize(textline, fontFace=font, fontScale=scale, thickness=1)\r\n",
        "    y += res[0][1]+res[1]\r\n",
        "  # Trip the image\r\n",
        "  while np.sum(image[:, :5]) == 0:\r\n",
        "    image = image[:, 2:]\r\n",
        "  while np.sum(image[:, -5:]) == 0:\r\n",
        "    image = image[:, :-2]\r\n",
        "  while np.sum(image[:5, :]) == 0:\r\n",
        "    image = image[2:, :]\r\n",
        "  while np.sum(image[-5:, :]) == 0:\r\n",
        "    image = image[:-2, :]\r\n",
        "  # Invert colours\r\n",
        "  image = 255-image\r\n",
        "  # Plot\r\n",
        "  plt.figure(figsize=(10, 10))\r\n",
        "  plt.imshow(image, cmap='gray')\r\n",
        "  plt.show()\r\n",
        "  original_image = image.copy()\r\n",
        "\r\n",
        "widgets.interact(render_text, font=fonts, scale=widgets.FloatSlider(min=.1, max=1.5, step=.1, value=1));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7LiBLkKOIbj"
      },
      "source": [
        "### Your own image\r\n",
        "\r\n",
        "You can use the following cell to load your own data. Upload a text file called ```my_transcription.txt``` and an image file ```my_transcription.png``` (or ```my_transcription.jpg```) to the virtual machine. Then run the cell. You can upload files using the file dialog to the left (where you have the table of contents).\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0768b4vvOIlR"
      },
      "source": [
        "if os.path.exists(\"my_transcription.txt\") and (os.path.exists(\"my_image.png\") or os.path.exists(\"my_image.jpg\")):\r\n",
        "  if os.path.exists(\"my_image.png\"):\r\n",
        "    original_image = cv2.imread(\"my_image.png\", cv2.IMREAD_GRAYSCALE)\r\n",
        "  else:\r\n",
        "    original_image = cv2.imread(\"my_image.jpg\", cv2.IMREAD_GRAYSCALE)\r\n",
        "  image = original_image.copy()\r\n",
        "\r\n",
        "  with open(\"my_transcription.txt\", 'r', encoding='utf-8') as file:\r\n",
        "      golden_transcription = file.read()\r\n",
        "  transcription = None\r\n",
        "  \r\n",
        "  print(\"Loaded\")\r\n",
        "else:\r\n",
        "  print(\"File(s) not found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr2BUzJshb5C"
      },
      "source": [
        "### Show the currently loaded data\r\n",
        " \r\n",
        "This shows whatever is loaded at the moment. If you were to run this code before loading any data, it would return an error and say that the data variable for the original image doesn't exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-6HlTKhb_O"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\r\n",
        "plt.imshow(original_image, cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.show()\r\n",
        "print(golden_transcription)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvpvbZQahTd"
      },
      "source": [
        "## Modify data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqWqxjZyOksq"
      },
      "source": [
        "### Degradation tool\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTNSeLf0h5sm"
      },
      "source": [
        "def resize_and_compress(scale, quality):\r\n",
        "  global image, original_image\r\n",
        "  # Resize the image\r\n",
        "  new_dimensions = (int(original_image.shape[1]*scale/100),\r\n",
        "                    int(original_image.shape[0]*scale/100))\r\n",
        "  original_dimensions = (original_image.shape[1], original_image.shape[0])\r\n",
        "  downscaled_image = cv2.resize(original_image, new_dimensions, interpolation=cv2.INTER_LANCZOS4)\r\n",
        "  # print(\"Resolution is %.1f Mpix (down from the original %.fMpix)\" % (np.prod(new_dimensions)/1e6, np.prod(original_dimensions)/1e6))\r\n",
        "  # Compress\r\n",
        "  result, compressed_image = cv2.imencode('.jpg', downscaled_image, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\r\n",
        "  decompressed_image = cv2.imdecode(compressed_image, 1)\r\n",
        "  # print(\"Compression ratio %.1f\" % (decompressed_image.nbytes/compressed_image.nbytes))\r\n",
        "  # print(\"Space saving %.1f%%\" % (100*(1-compressed_image.nbytes/decompressed_image.nbytes)))\r\n",
        "  # Show the image\r\n",
        "  image = decompressed_image\r\n",
        "  plt.figure(figsize=(7, 7))\r\n",
        "  plt.imshow(image)\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "widgets.interact(resize_and_compress, scale=widgets.IntSlider(min=1, max=100, step=1, value=100), quality=widgets.IntSlider(min=1, max=100, step=1, value=100));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Fom7k3aM5o"
      },
      "source": [
        "## Measure the OCR quality\r\n",
        "\r\n",
        "The following are some cells for visualising and analysing the OCR quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdRtK7xYI2Lb"
      },
      "source": [
        "### Running the OCR\r\n",
        " \r\n",
        "This code cell runs the OCR. Notice how short this code cell is. Only one line is needed to call the OCR engine. The rest of the code creates the widget which makes you able to choose the language model interactively (without changing the code)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlUWrLOLaM-5"
      },
      "source": [
        "def transcribe(lang):\r\n",
        "  global transcription, chosen_language_model\r\n",
        "  chosen_language_model = lang\r\n",
        "  print(\"Running OCR... \", end=\"\")\r\n",
        "  transcription = pytesseract.image_to_string(image, lang=lang)\r\n",
        "  print(\"done\")\r\n",
        "  print(transcription)\r\n",
        "\r\n",
        "widgets.interact(transcribe, lang=widgets.widgets.Dropdown(\r\n",
        "    options=language_models, value=language_models[0], description='Language:'));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzaEJoSKPk5Y"
      },
      "source": [
        "### Testing the OCR quality\n",
        " \n",
        "A common metric of quality of the OCR is the word/character error rate (WER/CER), i.e. the number of non-recognised words/characters in relation to the total number of words/characters. This can be done by flexibly matching the original text with the text returned from the OCR using the so-called Levenshtein distance. The Levenshtein distance is a distance metric between sequences of symbols where the distance is the number of edit operations required for transforming one sequence of symbols into another sequence. In our case, this is equal to how many characters or words need to be removed, added, or changed to transform one string into another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCA9UBXhPk5x"
      },
      "source": [
        "we = wer(golden_transcription, transcription)\n",
        "print(\"Word level errors\")\n",
        "print(golden_transcription.split())\n",
        "print(transcription.split())\n",
        "print(\"Word errors:\", we[0])\n",
        "print(\"WER: %.1f%%\" % (we[1]*100))\n",
        "print()\n",
        "ce = cer(golden_transcription, transcription)\n",
        "print(\"Character level errors\")\n",
        "print(list(golden_transcription))\n",
        "print(list(transcription))\n",
        "print(\"Character errors:\", ce[0])\n",
        "print(\"CER: %.1f%%\" % (ce[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kssrJpbI3I7o"
      },
      "source": [
        "### Trying WER and CER\r\n",
        "\r\n",
        "Change the strings in the widget below to experiment with word/character error rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rNPltL1qMA"
      },
      "source": [
        "def difference(text1, text2):\r\n",
        "  print(\"%i character level edit operations are needed\" % cer(text1, text2)[0])\r\n",
        "  print(\"%i word level edit operations are needed\" % wer(text1, text2)[0])\r\n",
        "\r\n",
        "widgets.interact(difference, \r\n",
        "                 text1=widgets.Text(value='This text is spelled correctly', placeholder='Type something', description='Text 1:'), \r\n",
        "                 text2=widgets.Text(value='This textis spleled corecty', placeholder='Type something2', description='Text 2:',));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFfucOkBDB-V"
      },
      "source": [
        "## Visualise the OCR results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tq7MJpzUvjN"
      },
      "source": [
        "### Show the character boxes\n",
        "\n",
        "The image given to tesseract goes though layout analysis (as discussed in the lecture) before returning strings and positions for the shapes (characters). We can get this data from tesseract if we ask for it. Below, this data is plotted on the original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ON9RC_bPk54"
      },
      "source": [
        "character_boxes = pytesseract.image_to_boxes(image, lang=chosen_language_model)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(image, cmap='gray')\n",
        "for entry in character_boxes.split(\"\\n\"):\n",
        "  if len(entry) > 0:\n",
        "    char, y1, x1, y2, x2, _ = entry.split()\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    x1 = image.shape[0]-int(x1)\n",
        "    x2 = image.shape[0]-int(x2)\n",
        "    plt.plot([y1, y2, y2, y1, y1], [x1, x1, x2, x2, x1]) # Plot the box\n",
        "    plt.text(y2, x1, char, color='m') # Plot the OCRed character\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhlgKHA-gTmR"
      },
      "source": [
        "### Show the boxes of the words and lines\n",
        "\n",
        "We can also ask Tesseract for the boxes for full words and lines (as it has detected them)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM-TMxn2-wPL"
      },
      "source": [
        "word_boxes = pytesseract.image_to_data(image, lang=chosen_language_model)\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(20, 10))\r\n",
        "ax = fig.subplots(1, 2)\r\n",
        "ax[0].imshow(image, cmap='gray')\r\n",
        "ax[1].imshow(image, cmap='gray')\r\n",
        "for i, entry in enumerate(word_boxes.split(\"\\n\")[1:]):\r\n",
        "  e = entry.split()\r\n",
        "  if len(e) >= 11:\r\n",
        "    level = int(e[0])\r\n",
        "    if int(level) == 4:\r\n",
        "      # Lines\r\n",
        "      _, _, _, _, _, _, y1, x1, y2, x2, _ = e\r\n",
        "      y1 = int(y1)\r\n",
        "      y2 = int(y2) + y1\r\n",
        "      x1 = int(x1)\r\n",
        "      x2 = int(x2) + x1\r\n",
        "      ax[1].plot([y1, y2, y2, y1, y1], [x1, x1, x2, x2, x1]) # Plot the box\r\n",
        "    elif level == 5 and len(e) == 12:\r\n",
        "      # Words\r\n",
        "      _, _, _, _, _, _, y1, x1, y2, x2, _, text = e\r\n",
        "      y1 = int(y1)\r\n",
        "      y2 = int(y2) + y1\r\n",
        "      x1 = int(x1)\r\n",
        "      x2 = int(x2) + x1\r\n",
        "      ax[0].plot([y1, y2, y2, y1, y1], [x1, x1, x2, x2, x1], color=\"C%i\" % i) # Plot the box\r\n",
        "      ax[0].text(y1, x1, text, color=\"C%i\" % i) # Plot the OCRed character\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}